#!/bin/bash

export nn1_host=$(machine_for_role nucleus 1 host)
export nn2_host=$(machine_for_role nucleus 2 host)

cat >> /opt/cell/cluster/cluster.yaml <<-EOT
### HDFS setup
hadoop_version: 2.6.0-cdh5.4.2-adobe
hadoop_lzo_version: 0.4.20
nn1_host: ${nn1_host}
nn2_host: ${nn2_host}
zk_quorum:
EOT

zk-list-nodes | sed 's/,/\n/g' | sed 's/:.*$//g' | sed 's/^/-  /' >> /opt/cell/cluster/cluster.yaml

cat >> /opt/cell/cluster/cluster.yaml <<-EOT
hadoop.dfs.nameservice_id: "${cell_name}"
hadoop_data_nodes: []
hadoop_number_of_disks: $(/usr/local/bin/detect-and-mount-disks)
hadoop::historyserver_host: "${nn1_host}"
hadoop::proxyusers:
  - name: root
    groups: ['*']
    hosts:  ['*']
hadoop_data_nodes: []
EOT

module_name="10-hdfs-raw"
if [[ -f /opt/cell/etc/roles/nucleus ]]; then
  /usr/local/bin/provision puppet base,hadoop_2_namenode,hadoop_2,hadoop_2_journalnode

  export host=$(hostname -f)
  service hadoop-hdfs-journalnode start
  sleep 10
  aws s3api put-object --bucket ${cell_bucket_name} --key ${full_cell_name}/shared/setup/${aws_parent_stack_name}/zk/$host
  if [[ $host == $nn1_host ]]; then
    aws_wait "aws s3api list-objects --bucket ${cell_bucket_name} --prefix ${full_cell_name}/shared/setup/${aws_parent_stack_name}/zk/" ".Contents | length" "3"
    su --login hadoop -c "/home/hadoop/hadoop/bin/hdfs namenode -format -nonInteractive" &> hdfs-format-result
    format_successful=$?
    if [[ format_successful -eq 1 ]]; then
        # check if already formatted or otherwise fail
        grep -q "Running in non-interactive mode, and data appears to exist in Storage Directory" hdfs-format-result
        if [[ $? -ne 0 ]]; then
            cat hdfs-format-result
            exit 1 # provision script will mark as failed and retry
        fi
    fi

    # this only "formats" the zookeeper hadoop-ha dir if it doesn't exist
    # hence it would only fail if zk is down
    su --login hadoop -c "/home/hadoop/hadoop/bin/hdfs zkfc -formatZK -nonInteractive"
    systemctl start hadoop-hdfs-zkfc
    systemctl start hadoop-hdfs-namenode
    aws s3api put-object --bucket ${cell_bucket_name} --key ${full_cell_name}/shared/setup/${aws_parent_stack_name}/nn1
  fi

  if [[ $host == $nn2_host ]]; then
    aws_wait "aws s3api list-objects --bucket ${cell_bucket_name} --prefix ${full_cell_name}/shared/setup/${aws_parent_stack_name}/zk/" ".Contents | length" "3"
    aws_wait "aws s3api list-objects --bucket ${cell_bucket_name} --prefix ${full_cell_name}/shared/setup/${aws_parent_stack_name}/nn1" ".Contents | length" "1"
    su --login hadoop -c "/home/hadoop/hadoop/bin/hdfs namenode -bootstrapStandby -nonInteractive"
    systemctl start hadoop-hdfs-zkfc
    systemctl start hadoop-hdfs-namenode
    sleep 10
    aws s3api put-object --bucket ${cell_bucket_name} --key ${full_cell_name}/shared/setup/${aws_parent_stack_name}/nn2
  fi
elif [[ -f /opt/cell/etc/roles/stateful-body ]]; then
  /usr/local/bin/provision puppet base,hadoop_2_datanode

  aws_wait "aws s3api list-objects --bucket ${cell_bucket_name} --prefix ${full_cell_name}/shared/setup/${aws_parent_stack_name}/nn2" ".Contents | length" "1"
  systemctl start hadoop-hdfs-datanode
else
  report_status "${module_name} skipped"
fi
