server {
  listen 80;
  server_name ~^hdfs.(api|gw)\.(?<domain>.+);
  set $hdfs_http_port 50070;
  set_by_lua $hdfs_nn1 'return os.getenv("HDFS_NN1_HOST") or "10.255.255.2"';
  set_by_lua $hdfs_nn2 'return os.getenv("HDFS_NN2_HOST") or "10.255.255.2"';
  set $hdfs_active_nn $hdfs_nn1;

  include /etc/api-gateway/conf.d/includes/resolvers.conf;

  location / {
    if ( $is_admin_zone_ip = "0" ) {
        return 403;
    }

    rewrite_by_lua_block {
      local hdfs = require 'hdfs'
      ngx.var.hdfs_active_nn = hdfs.find_active_namenode()
    }

    proxy_pass http://$hdfs_active_nn:$hdfs_http_port;
  }

  # https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/WebHDFS.html
  # Web HDFS is a rest endpoint for HDFS operations, to make it easier to clients to connect to a HDFS clients from outside the cell, through the Gateway
  # Due to some complications in the web HDFS design (some requests are translated to redirects to internal cell machines), we need to proxy them through Gateway
  location /webhdfs {
    if ( $is_admin_zone_ip = "0" ) {
        return 403;
    }

    set $webhdfs_target_url "";

    access_by_lua_block {
      local hdfs = require 'hdfs'
      local err, res = hdfs.get_webhdfs_target_url()
      if err ~= nil then
          ngx.log(ngx.ERR, "[webhdfs] ERROR: ", err)
          ngx.header.content_type = "application/json"
          ngx.say(err)
          ngx.exit(ngx.HTTP_NOT_ALLOWED)
      else
          ngx.var.webhdfs_target_url = res
      end
    }

    proxy_pass $webhdfs_target_url;
  }

  location /cellos/config {
    if ( $is_admin_zone_ip = "0" ) {
        return 403;
    }
    access_by_lua_block {
        local hdfs = require "hdfs"
        return hdfs.exec_config()
    }
  }

  location ~* /cellos/config/(?<hdfs_config_file>.+) {
    if ( $is_admin_zone_ip = "0" ) {
        return 403;
    }

    access_by_lua_block {
        local hdfs = require "hdfs"
        return hdfs.exec_config(ngx.var.hdfs_config_file)
    }
  }
}

