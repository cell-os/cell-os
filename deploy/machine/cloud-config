#cloud-config
packages:
  - curl
  - wget
  - ruby
  - ruby-devel
  - kpartx
  - parted
  - cifs-utils
runcmd:
  - "wget https://github.com/stedolan/jq/releases/download/jq-1.5/jq-linux64 -O /usr/local/bin/jq"
  - "chmod +x /usr/local/bin/jq"
mounts: 
  - [ ephemeral0, None ]
write_files:
  - path: /usr/local/bin/cmdwait
    owner: 'root:root'
    permissions: '0755'
    content: |
      #!/bin/bash
      # executes a command ($1) until its output is equal to the target ($2)
      [[ $# = 2 ]] || { echo "Internal error calling wait-for" ; exit 99 ; }
      cmd=$1
      target=$2
      loop=1
      echo "Waiting for $cmd"
      while [[ $loop -le 200 ]]; do
          STATE=$(eval $cmd)
          if [[ "$STATE" == "$target" ]]; then
              exit 0
          fi
          sleep 5
          printf "."
          loop=$(( $loop + 1 ))
      done
      exit 1
  - path: /usr/local/bin/zk-list-nodes
    owner: 'root:root'
    permissions: '0755'
    content: |
      #!/bin/bash
      #list the zookeeper nodes, making sure that we have at least 3
      source /etc/profile.d/cellos.sh
      num_servers=${num_servers:-3}
      code=$(curl -s -o /dev/null -w "%{http_code}" ${zk_cluster_list})
      if (( code == 200 )); then
        # we have a 200, get the servers
        found=$(curl -H "Accept: application/json" ${zk_cluster_list} 2>/dev/null | jq ".servers | length")
        if (( $? == 0 )); then
          if (( $found < $num_servers )); then
            >&2 echo "not enough servers found ($found out of $num_servers)"
            exit 1
          fi
        else
          >&2 echo no servers found
          exit 1
        fi
      else
        >&2 echo no servers found
        exit 1
      fi

      curl -H "Accept: application/json" ${zk_cluster_list} 2>/dev/null | jq -r '(.port | tostring) as $port | .servers | map(. + ":" + $port) | join(",")'
  - path: /usr/local/bin/zk-barrier
    owner: 'root:root'
    permissions: '0755'
    content: |
      #!/bin/bash
      #blocks execution until we have a ZK quorum
      source /etc/profile.d/cellos.sh
      num_servers=3
      while true; do
        code=$(curl -H "Accept: application/json" -s -o /dev/null -w "%{http_code}" "${zk_base_url}/cluster/status")
        if (( code == 200 )); then
          num_serving=$(curl -H "Accept: application/json" "${zk_base_url}/cluster/status" 2>/dev/null | jq '[.[] | select(.description == "serving")] | length')
          num_serving=${num_serving:-0}
          found=$(curl -H "Accept: application/json" ${zk_cluster_list} 2>/dev/null | jq ".servers | length")
          found=${found:-0}

          if (( $num_serving >= $num_servers && $found >= $num_servers )); then
            # check servers
            valid=()
            for hp in $(zk-list-nodes | sed 's/,/\n/g'); do
              host=$(echo $hp | sed 's/:.*$//')
              exec 6<>/dev/tcp/$host/2181 && valid+=($host)
            done
            if (( ${#valid[@]}  >= $num_servers )); then
              break
            else
              echo -n "."
              sleep 1
            fi
          else
            echo -n "."
            sleep 1
          fi
        else
          echo -n "."
          sleep 1
        fi
      done
  - path: /usr/local/bin/provision
    owner: 'root:root'
    permissions: '0755'
    content: |
      #!/bin/bash
      # executes a provision command with retries on failure
      source /etc/profile.d/cellos.sh

      provision_with_retry() {
        local roles=${1?"usage provision_with_retry <roles> [retries]"}
        local module_name=${2:${roles}}
        local max_attempts=${3:-5}
        local attempt=0
        local status=-1
        until [[ $status == 0 ]]; do
          bash /usr/local/bin/saasbase_installer -v -d /opt/cell -m /opt/cell/cluster/puppet/modules run-puppet /opt/cell/cluster --roles $roles
          status=$?
          attempt=$(($attempt + 1))
          if [[ $attempt -gt $max_attempts ]]; then
            exit 1
          fi
          if [[ $status != 0 ]]; then
            echo "Retry ${1} provisioning step: exit code $status, attempt $attempt / $max_attempts"
            report_status "${module_name} retry"
          fi
        done
      }

      if [[ $1 == "puppet" ]]; then
          shift
          AWS_ACCESS_KEY_ID="${SAASBASE_ACCESS_KEY_ID}" AWS_SECRET_ACCESS_KEY="${SAASBASE_SECRET_ACCESS_KEY}" \
          provision_with_retry $@
      fi
  - path: /usr/local/bin/detect-and-mount-disks
    owner: 'root:root'
    permissions: '0755'
    content: |
      #!/bin/bash
      #detects and mounts /dev/sdx disks on an instance
      source /etc/profile.d/cellos.sh
      if [[ "${cell_backend}" == "aws" ]]; then
        deviceslist=('/dev/xvdb' '/dev/xvdc' '/dev/xvdd' '/dev/xvde' '/dev/xvdf' '/dev/xvdg' '/dev/xvdh' '/dev/xvdi' '/dev/xvdj' '/dev/xvdk' '/dev/xvdl' '/dev/xvdm' '/dev/xvdn' '/dev/xvdo' '/dev/xvdp' '/dev/xvdq' '/dev/xvdr' '/dev/xvds' '/dev/xvdt' '/dev/xvdu' '/dev/xvdv' '/dev/xvdw' '/dev/xvdx' '/dev/xvdy' '/dev/xvdz')
        for device in ${deviceslist[@]}; do
            if ([ -b $device ]) then
                actual_devices=( "${actual_devices[@]}" $device )
            fi
        done

        n=0
        for device in "${actual_devices[@]}"; do
            n=`expr $n + 1`
            if [ ! -b ${device}1 ]; then
                echo "Creating partition on ${device}" >&2
                parted -s -a optimal $device mklabel gpt -- mkpart primary xfs 1 -1 >&2
                partprobe $device >&2 || true
                mkfs.xfs ${device}1 >&2 || true
            fi
            if mountpoint -q -- "/mnt/data_${n}"; then
                echo "/mnt/data_${n} is already mounted" >&2
            else
              echo "/mnt/data_${n} is not a mountpoint" >&2
                mkdir -p -m 755 /mnt/data_${n} >&2
                mount ${device}1 /mnt/data_${n} >&2 || true
                echo "${device}1 /mnt/data_${n} xfs rw,relatime,attr2,inode64,noquota,nofail 0 0" >> /etc/fstab
            fi

            mkdir -p -m 755 /mnt/data_${n}/{hadoop_data,kafka_data}
            chown -R hadoop:hadoop /mnt/data_${n}/{hadoop_data,kafka_data}
        done
        echo $n
      fi
  - path: /usr/local/bin/get_ip
    owner: 'root:root'
    permissions: '0755'
    content: |
      #!/bin/bash
      #gets the local machine ip
      source /etc/profile.d/cellos.sh

      if [[ "$cell_backend" == "aws" ]]; then
        echo $(curl -fsSL http://169.254.169.254/latest/meta-data/local-ipv4)
      fi
  - path: /usr/local/bin/report_status
    owner: 'root:root'
    permissions: '0755'
    content: |
      #!/bin/bash
      #copies an updated status file in a central location
      source /etc/profile.d/cellos.sh
      mkdir -p /opt/cell/status

      message=$@
      ts=$(date +"%s")
      status_file=/opt/cell/status/${instance_id}.json

      echo -e "${message} ${ts}" | tee -a $status_file
      
      filetool --put $status_file /shared/status/${instance_id} \
          --metadata-directive REPLACE --cache-control max-age=0,public \
          --expires 2000-01-01T00:00:00Z &>/dev/null
  - path: /usr/local/bin/yaml2json
    owner: 'root:root'
    permissions: '0755'
    content: |
      #!/bin/bash
      #transforms yaml file (stdin) to json(stdout)
      set -o pipefail

      python -c 'import sys, yaml, json; json.dump(yaml.load(sys.stdin), sys.stdout, indent=4)' | cat "$@"
  - path: /usr/local/bin/machine_for_role
    owner: 'root:root'
    permissions: '0755'
    content: |
      #!/bin/bash
      # This script returns either the ip or the host for a cell machine in a certain
      # role
      # Usage: machine_for_role <role> <index> <host|ip>
      # - role: nucleus, stateful-body, stateless-body, membrane
      # - index: starting from 1
      # - host|ip
      # FIXME: we assume describe instances always returns the instances in the ASG
      #        order; this might not be the case (when instances break, for example)
      source /etc/profile.d/cellos.sh
      role=$1
      index=$(( $2 - 1 ))
      if [[ "$cell_backend" == "aws" ]]; then
        [[ "$3" == "ip" ]] && ret=0
        [[ "$3" == "host" ]] && ret=1
        machine=$(aws --region ${aws_region} ec2 describe-instances --query 'Reservations[*].Instances[*].[PrivateIpAddress, PrivateDnsName]' --filters Name=instance-state-code,Values=16 Name=tag:cell,Values=${cell_name} Name=tag:role,Values=${role} | jq -r ".[0][${index}][${ret}]")
        echo $machine
        fi
  - path: /usr/local/bin/error_exit
    owner: 'root:root'
    permissions: '0755'
    content: |
      #!/bin/bash
      source /etc/profile.d/cellos.sh
      [[ "${cell_backend}" == "aws" ]] && cfn-signal -e 1 -r "$1" "$aws_wait_handle"
      exit 1
  - path: /usr/local/bin/success_exit
    owner: 'root:root'
    permissions: '0755'
    content: |
      #!/bin/bash
      source /etc/profile.d/cellos.sh
      [[ $cell_backend == "aws" ]] && cfn-signal -e 0 -r "Stack setup complete" "${aws_wait_handle}"
      exit 0
  - path: /usr/local/bin/pathtool
    owner: 'root:root'
    permissions: '0755'
    content: |
      #!/bin/bash
      #displays the central shared location path
      source /etc/profile.d/cellos.sh
      path=$1
      [[ "${cell_backend}" == "aws" ]] && echo -n "s3://${cell_bucket_name}/${full_cell_name}${path}"
  - path: /usr/local/bin/filetool
    owner: 'root:root'
    permissions: '0755'
    content: |
      #!/bin/bash
      #implements operations on the central shared file location: get, put, sync, touch; transforms logical paths to physical paths 
      source /etc/profile.d/cellos.sh
      case "$1" in
        --touch)
          dest=$2
          [[ "${cell_backend}" == "aws" ]] && aws s3api put-object --bucket ${cell_bucket_name} --key ${full_cell_name}${dest}
        ;;
        --sync)
          src=$2
          full_dest=$3
          [[ "${cell_backend}" == "aws" ]] && aws s3 sync --delete "${src}" "${full_dest}"
        ;;
        --count)
          path=$2
          [[ "${cell_backend}" == "aws" ]] && aws s3api list-objects --bucket ${cell_bucket_name} --prefix ${full_cell_name}${path} | jq -r '.Contents | length'
        ;;
        --put)
          shift; src=$1
          shift; dest=$1
          shift
          [[ "${cell_backend}" == "aws" ]] && aws s3 cp ${src} s3://${cell_bucket_name}/${full_cell_name}${dest} "$@"
          [[ "${cell_backend}" == "azure" ]] && cp -a ${src} /cell${dest}
        ;;
        --get)
          src=$2
          dest=$3
          [[ "${cell_backend}" == "aws" ]] && aws s3 cp s3://${cell_bucket_name}/${full_cell_name}${src} ${dest}
        ;;
        *)
        echo "filetool [--sync|--touch] [src] [dest]"
        exit 1
        ;;
      esac
  - path: /usr/local/bin/prepare_aws
    owner: 'root:root'
    permissions: '0755'
    content: |
      #!/bin/bash
      source /etc/profile.d/cellos.sh
      curl https://s3.amazonaws.com/aws-cloudwatch/downloads/latest/awslogs-agent-setup.py -O
      cat >> awslogs.conf <<-EOT
      [general]
      state_file = /var/awslogs/state/agent-state

      [/var/log/cloud-init-output.log]
      datetime_format = %Y-%m-%d %H:%M:%S
      file = /var/log/cloud-init-output.log
      buffer_duration = 5000
      log_stream_name = ${full_cell_name}
      initial_position = start_of_file
      log_group_name = /var/log/cloud-init-output.log
      EOT

      python ./awslogs-agent-setup.py -r ${aws_region} -n -c awslogs.conf
  - path: /usr/local/bin/prepare_seeds
    owner: 'root:root'
    permissions: '0755'
    content: |
      #!/bin/bash
      #sets up the seed and provisioning folder (/opt/cell)
      #copies the seed profile, versions bundle from the central shared location
      source /etc/profile.d/cellos.sh
      mkdir -p /opt/cell/etc/roles /opt/cell/cluster/puppet/modules /opt/cell/puppet/profiles /opt/cell/seed
      download_cell_profile() {
        AWS_ACCESS_KEY_ID="${SAASBASE_ACCESS_KEY_ID}" AWS_SECRET_ACCESS_KEY="${SAASBASE_SECRET_ACCESS_KEY}" aws s3 cp ${repository}/cell-os/${cellos_version}.yaml /opt/cell/puppet/profiles
        # attempt to override the profile from the local bucket
        filetool --get /shared/cell-os/${cellos_version}.yaml /opt/cell/puppet/profiles/
        echo ${cellos_version} > /opt/cell/cluster/profile
        touch /opt/cell/cluster/cluster.yaml
      }
      # download cell profile so we can get the saasbase installer version
      download_cell_profile
      touch /opt/cell/etc/roles/${cell_role}
      saasbase_version=$(cat /opt/cell/puppet/profiles/${cellos_version}.yaml | yaml2json | jq -r '.["saasbase_deployment::version"]')
      echo "export saasbase_version=${saasbase_version}" >> /etc/profile.d/cellos.sh
      # download sb installer
      curl -o /usr/local/bin/saasbase_installer https://s3.amazonaws.com/${repository:5}/saasbase_installer${saasbase_version}
      chmod +x /usr/local/bin/saasbase_installer

      AWS_ACCESS_KEY_ID="${SAASBASE_ACCESS_KEY_ID}" AWS_SECRET_ACCESS_KEY="${SAASBASE_SECRET_ACCESS_KEY}" bash /usr/local/bin/saasbase_installer -d /opt/cell fetch ${saasbase_version}
      download_cell_profile

      AWS_ACCESS_KEY_ID="${SAASBASE_ACCESS_KEY_ID}" AWS_SECRET_ACCESS_KEY="${SAASBASE_SECRET_ACCESS_KEY}" aws s3 cp ${repository}/cell-os/seed-${cellos_version}.tar.gz /opt/cell
      # attempt to download seed from local bucket as well
      filetool --get /shared/cell-os/seed.tar.gz /opt/cell/
      # unarchive files
      mkdir -p /opt/cell/seed
      pushd /opt/cell
      rm -rf seed
      [[ -f seed-${cellos_version}.tar.gz ]] && tar zxf seed-${cellos_version}.tar.gz
      [[ -f seed.tar.gz ]] && tar zxf seed.tar.gz
      rm -rf seed*.tar.gz
      popd
    
