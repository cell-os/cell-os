#!/bin/bash
# TODO add a header describing the overall idea and plan of improvement (proper cloud-init, etc.)
# default images mount the first ephemeral disk to /mnt
# this is from cloud-init config that comes with the cloud-init.rpm
umount /mnt
# set timezone
echo "UTC" > /etc/timezone
ln -sf /usr/share/zoneinfo/UTC /etc/localtime

# Install mustache gem for easy templating
gem install mustache

# Pip
curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
python get-pip.py
# Downgrade pip to avoid https://github.com/pypa/pip/issues/3384
pip install "pip==8.1.0"

# pystache
easy_install pystache

# aws-cli
pip install --upgrade "awscli==1.9.21"

# pip install awscli installs certify - that deprecates some crypto shit and things don't work well
# TODO check if upgrading this fixes it
pip uninstall -y certifi
pip install certifi==2015.04.28
pip install pyyaml

source /etc/profile.d/cellos.sh
# Helper functions
function error_exit
{
  cfn-signal -e 1 -r "$1" "$aws_wait_handle"
  exit 1
}

# export vars
echo "export number_of_disks=$(/usr/local/bin/detect-and-mount-disks)" >> /etc/profile.d/cellos.sh
source /etc/profile.d/cellos.sh

report_status "role ${cell_role}"
report_status "seeds ${cell_modules},zk_barrier"
report_status "${cell_role} start"

# install awslogs
curl https://s3.amazonaws.com/aws-cloudwatch/downloads/latest/awslogs-agent-setup.py -O
cat >> awslogs.conf <<-EOT
[general]
state_file = /var/awslogs/state/agent-state

[/var/log/cloud-init-output.log]
datetime_format = %Y-%m-%d %H:%M:%S
file = /var/log/cloud-init-output.log
buffer_duration = 5000
log_stream_name = ${full_cell_name}
initial_position = start_of_file
log_group_name = /var/log/cloud-init-output.log
EOT

python ./awslogs-agent-setup.py -r ${aws_region} -n -c awslogs.conf

# prepare roles
mkdir -p /opt/cell/etc/roles/
touch /opt/cell/etc/roles/${cell_role}

# prepare provisioning - cell specific
download_cell_profile() {
    mkdir -p /opt/cell/cluster/puppet/modules
    mkdir -p /opt/cell/puppet/profiles
    AWS_ACCESS_KEY_ID="${SAASBASE_ACCESS_KEY_ID}" AWS_SECRET_ACCESS_KEY="${SAASBASE_SECRET_ACCESS_KEY}" aws s3 cp ${repository}/cell-os/${cellos_version}.yaml /opt/cell/puppet/profiles
    # attempt to override the profile from the local bucket
    aws s3 cp s3://${cell_bucket_name}/${full_cell_name}/shared/cell-os/${cellos_version}.yaml /opt/cell/puppet/profiles/
    echo ${cellos_version} > /opt/cell/cluster/profile
    touch /opt/cell/cluster/cluster.yaml
}

# download cell profile so we can get the saasbase installer version
download_cell_profile

# download saasbase_installer (for now, we are expecting S3-based storage)
if [ ${repository:0:5} != "s3://" ]; then
  echo "The 'repository' var must point to an S3 bucket"
  exit 1
fi
saasbase_version=$(cat /opt/cell/puppet/profiles/${cellos_version}.yaml | yaml2json | jq -r '.["saasbase_deployment::version"]')
echo "export saasbase_version=${saasbase_version}" >> /etc/profile.d/cellos.sh

curl -o /usr/local/bin/saasbase_installer https://s3.amazonaws.com/${repository:5}/saasbase_installer${saasbase_version}
chmod +x /usr/local/bin/saasbase_installer

AWS_ACCESS_KEY_ID="${SAASBASE_ACCESS_KEY_ID}" AWS_SECRET_ACCESS_KEY="${SAASBASE_SECRET_ACCESS_KEY}" bash /usr/local/bin/saasbase_installer -d /opt/cell fetch ${saasbase_version}
# provision seed
# we need to download the cell profile again, because saasbase_installer fetch
# overwrites all the files in /opt/cell/puppet/profiles
download_cell_profile
AWS_ACCESS_KEY_ID="${SAASBASE_ACCESS_KEY_ID}" AWS_SECRET_ACCESS_KEY="${SAASBASE_SECRET_ACCESS_KEY}" aws s3 cp ${repository}/cell-os/seed-${cellos_version}.tar.gz /opt/cell

# attempt to download seed from local bucket as well
aws s3 cp s3://$cell_bucket_name/${full_cell_name}/shared/cell-os/seed.tar.gz /opt/cell

####################### provision
mkdir -p /opt/cell/seed
pushd /opt/cell
rm -rf seed
[[ -f seed-${cellos_version}.tar.gz ]] && tar zxf seed-${cellos_version}.tar.gz
[[ -f seed.tar.gz ]] && tar zxf seed.tar.gz
rm -rf seed*.tar.gz
popd

function do_provision {
    local provision_script="${1}"
    for module_path in /opt/cell/seed/*; do
        local module="$(basename $module_path)"
        if [[ "$cell_modules" == *"${module}"* ]]; then
            if [[ -x "${module_path}/${provision_script}" ]]; then
                report_status "${module} start"
                "${module_path}/${provision_script}" || {
                    report_status "${module} failed"
                    error_exit "Failed to deploy ${module}"
                }
                report_status "${module} end"
            fi
        fi
    done
}

# execute pre
do_provision provision

# wait for zk
report_status "zk_barrier start"
/usr/local/bin/zk-barrier
export zk=`zk-list-nodes 2>/dev/null`
report_status "zk_barrier end"

# execute post
do_provision provision_post

report_status "${cell_role} end"

# All is well so signal success
cfn-signal -e 0 -r "Stack setup complete" "${aws_wait_handle}"

#EOF
exit 0
