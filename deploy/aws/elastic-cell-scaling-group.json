{
  "AWSTemplateFormatVersion" : "2010-09-09",
  "Description" : "cell-os-base - https://git.corp.adobe.com/metal-cell/cell-os",

  "Parameters" : {
    "InstanceType" : {
      "Description" : "EC2 instance type",
      "Type" : "String",
      "Default" : "c3.2xlarge",
      "AllowedValues" : [
        "t2.micro", "t2.small", "t2.medium",
        "m3.medium", "m3.large", "m3.xlarge", "m3.2xlarge",
        "c4.large", "c4.xlarge", "c4.2xlarge", "c4.4xlarge", "c4.8xlarge",
        "c3.large", "c3.xlarge", "c3.2xlarge", "c3.4xlarge", "c3.8xlarge",
        "r3.large", "r3.xlarge", "r3.2xlarge", "r3.4xlarge", "r3.8xlarge",
        "i2.xlarge", "i2.2xlarge", "i2.4xlarge", "i2.8xlarge",
        "hs1.8xlarge", "g2.2xlarge"
      ],
      "ConstraintDescription" : "must be a valid, HVM-compatible EC2 instance type."
    },
    "CellName" : {
      "Description" : "The name of this cell (e.g. cell-1). This will get prefixed with account id and region to get the full cell id.",
      "Type" : "String",
      "Default": "cell-1"
    },
    "BucketName" : {
      "Description" : "Cell's S3 bucket name. Used for metadata and backups. Can be one per account as we prefix data with cell name inside",
      "Type" : "String"
    },
    "CellOsVersionBundle" : {
      "Description" : "cell-os bundle version",
      "Type" : "String",
      "Default": "cell-os-base-1.1-SNAPSHOT"
    },
    "KeyName" : {
      "Description" : "Existing EC2 KeyPair to be associated with all cluster instances for SSH access. The default user is 'centos'",
      "Type" : "AWS::EC2::KeyPair::KeyName"
    },
    "GroupSize" : {
      "Description" : "Number of nodes in the scaling group",
      "Type" : "Number",
      "Default": 1
    },
    "LoadBalancerNames" : {
      "Description" : "List of ELBs that ",
      "Type" : "CommaDelimitedList"
    },
    "ZookeeperLoadBalancer" : {
      "Description" : "ZK ELB Endpoint",
      "Type" : "String"
    },
    "Subnet" : {
      "Description" : "Subnet",
      "Type" : "AWS::EC2::Subnet::Id"
    },
    "Tags" : {
      "Description" : "Comma separated list of tags",
      "Type" : "CommaDelimitedList"
    },
    "ImageId" : {
      "Description" : "AMI ID",
      "Type" : "AWS::EC2::Image::Id"
    },
    "SecurityGroups" : {
      "Description" : "AMI ID",
      "Type" : "List<AWS::EC2::SecurityGroup::Id>"
    },
    "IamInstanceProfile" : {
      "Description" : "IAM Profile if any",
      "Type" : "String",
      "Default": ""
    },
    "Role" : {
      "Description" : "The role of the autoscaling group (stateless-body, membrane, etc.)",
      "Type" : "String"
    },
    "ParentStackName" : {
      "Description" : "",
      "Type" : "String"
    },
    "WaitHandle" : {
      "Description" : "",
      "Type" : "String"
    },
    "AssociatePublicIpAddress" : {
      "Description" : "AMI ID",
      "Type" : "String",
      "AllowedValues" : ["true", "false"],
      "Default" : "false"
    },
    "PreZkModules" : {
      "Description" : "Comma separated list of modules that don't require a running zk (e.g. docker,java) ",
      "Type" : "String"
    },
    "PostZkModules" : {
      "Description" : "Comma separated list of modules that require a running zk (e.g. hdfs, mesos::slave)",
      "Type" : "String"
    },
    "SaasBaseDeploymentVersion" : {
      "Description" : "saasbase-deployment version",
      "Type" : "String"
    },
    "SaasBaseAccessKeyId" : {
      "Type" : "String",
      "Description" : "SaasBase S3 repo read-only AWS account Access Key ID (http://saasbase.corp.adobe.com/ops/operations/deployment.html)"
    },
    "SaasBaseSecretAccessKey": {
      "Type" : "String",
      "Description" : "SaasBase S3 repo read-only AWS account Secret Access Key (http://saasbase.corp.adobe.com/ops/operations/deployment.html)"
    },
    "SaasBaseUserData": {
      "Type" : "String",
      "Description" : "Run after ZK quorum is found and before starting deployment. Base64, new line delimited string"
    },
    "SaasBaseUserDataPost": {
      "Type" : "String",
      "Description" : "Run after ZK quorum is found and before starting deployment. Base64, new line delimited string", 
      "Default": ""
    }
  },

  "Conditions" : {
    "HasIamInstanceProfile" : { "Fn::Not": [{ "Fn::Equals" : [{ "Ref" : "IamInstanceProfile" }, ""] }] }
  },

  "Resources" : {
    "Body" : {
      "Type" : "AWS::AutoScaling::AutoScalingGroup",
      "Properties" : {
        "LaunchConfigurationName" : { "Ref" : "BodyLaunchConfig" },
        "MinSize" : "0",
        "MaxSize" : "1000",
        "DesiredCapacity" : { "Ref" : "GroupSize" },
        "LoadBalancerNames" : { "Ref" : "LoadBalancerNames" },
        "VPCZoneIdentifier" : [{ "Ref" : "Subnet" }],
        "Tags" : [
          { "Key" : "role", "Value" : {"Ref": "Role"},     "PropagateAtLaunch" : "true" },
          { "Key" : "cell", "Value" : {"Ref": "CellName"}, "PropagateAtLaunch" : "true" }
        ]
      }
    },
    
    "BodyLaunchConfig" : {
      "Type" : "AWS::AutoScaling::LaunchConfiguration",
      "Metadata" : {
        "AWS::CloudFormation::Init" : {
          "config": {
            "files" : {
              "/usr/local/bin/jq": {
                "source": "https://github.com/stedolan/jq/releases/download/jq-1.5/jq-linux64", 
                "mode": "000755"
              }, 
              "/etc/profile.d/cellos.sh": {
                "content" : { "Fn::Join" : ["", [
                "#!/bin/bash\n", 
                "export zk_base_url=\"{{zk_base_url}}\"\n", 
                "export zk_discovery_url=\"{{zk_discovery_url}}\"\n", 
                "export aws_stack_name=\"{{aws_stack_name}}\"\n",
                "export aws_parent_stack_name=\"{{aws_parent_stack_name}}\"\n",
                "export aws_region=\"{{aws_region}}\"\n",
                "export SAASBASE_ACCESS_KEY_ID=\"{{saasbase_access_key_id}}\"\n",
                "export SAASBASE_SECRET_ACCESS_KEY=\"{{saasbase_secret_access_key}}\"\n",
                "export saasbase_version=\"{{saasbase_version}}\"\n",
                "export cellos_version=\"{{cellos_version}}\"\n",
                "export cell_bucket_name=\"{{cell_bucket_name}}\"\n", 
                "export cell_name=\"{{cell_name}}\"\n", 
                "export cell_role=\"{{cell_role}}\"\n", 
                "export instance_id=`wget -qO- http://169.254.169.254/latest/meta-data/instance-id`\n", 
                "export aws_region=`wget -qO- http://169.254.169.254/latest/meta-data/placement/availability-zone | sed 's/.$//'`\n"
                ]]}, 
                "context" : {
                  "zk_base_url"  : { "Fn::Join" : ["", [
                    "http://", { "Ref" : "ZookeeperLoadBalancer" }, "/exhibitor/v1"
                  ]]}, 
                  "zk_discovery_url"  : { "Fn::Join" : ["", [
                    "http://", { "Ref" : "ZookeeperLoadBalancer" }, "/exhibitor/v1/cluster/list"
                  ]]}, 
                  "aws_stack_name": { "Ref": "AWS::StackName" }, 
                  "aws_parent_stack_name": { "Ref": "ParentStackName" }, 
                  "aws_region": { "Ref": "AWS::Region" }, 
                  "saasbase_access_key_id": { "Ref": "SaasBaseAccessKeyId" }, 
                  "saasbase_secret_access_key": { "Ref": "SaasBaseSecretAccessKey" }, 
                  "saasbase_version": { "Ref": "SaasBaseDeploymentVersion" }, 
                  "cellos_version": { "Ref": "CellOsVersionBundle" }, 
                  "cell_bucket_name": { "Ref": "BucketName" }, 
                  "cell_name": { "Ref": "CellName" }, 
                  "cell_role": { "Ref": "Role" }, 
                  "cell_tags": { "Ref": "Tags" }
                }
              }, 
              "/usr/local/bin/aws_wait": {
                "content" : { "Fn::Join" : ["", [
                  "#!/bin/bash\n", 
                  "[[ $# = 3 ]] || { echo \"Internal error calling wait-for\" ; exit 99 ; }\n", 
                  "cmd=$1\n", 
                  "pattern=$2\n", 
                  "target=$3\n", 
                  "loop=1\n", 
                  "echo \"Waiting for $cmd | jq \\\"$pattern\\\"\"\n", 
                  "while [[ $loop -le 200 ]]; do\n", 
                  "    STATE=$($cmd | jq \"$pattern\")\n", 
                  "    if [[ \"$STATE\" == \"$target\" ]]; then\n", 
                  "        exit 0\n", 
                  "    fi\n", 
                  "    sleep 5\n", 
                  "    printf \".\"\n", 
                  "    loop=$(( $loop + 1 ))\n", 
                  "done\n", 
                  "exit 1\n"
                ]]}, 
                "mode": "000755"
              }, 
              "/usr/local/bin/zk-list-nodes" : {
                "content" : { "Fn::Join" : ["", [
                  "#!/bin/bash\n", 
                  "source /etc/profile.d/cellos.sh\n",
                  "num_servers=${num_servers:-0}\n", 
                  "code=$(curl -s -o /dev/null -w \"%{http_code}\" ${zk_discovery_url})\n", 
                  "if (( code == 200 )); then\n", 
                  "  # we have a 200, get the servers\n", 
                  "  found=$(curl -H \"Accept: application/json\" ${zk_discovery_url} 2>/dev/null | jq \".servers | length\")\n", 
                  "  if (( $? == 0 )); then\n", 
                  "    if (( $found < $num_servers )); then\n", 
                  "      >&2 echo \"not enough servers found ($found out of $num_servers)\"\n", 
                  "      exit 1\n", 
                  "    fi\n", 
                  "  else\n", 
                  "    >&2 echo no servers found\n", 
                  "    exit 1\n", 
                  "  fi\n", 
                  "else\n", 
                  "  >&2 echo no servers found\n", 
                  "  exit 1\n", 
                  "fi\n", 
                  "\n", 
                  "curl -H \"Accept: application/json\" ${zk_discovery_url} 2>/dev/null | jq -r '(.port | tostring) as $port | .servers | map(. + \":\" + $port) | join(\",\")'\n", 
                  ""
                ]]},
                "mode"    : "000755",
                "owner"   : "root",
                "group"   : "root"
              },
              "/usr/local/bin/zk-barrier" : {
                "content" : { "Fn::Join" : ["", [
                  "#!/bin/bash\n", 
                  "### Block until we have a ZK quorum\n", 
                  "num_servers=3\n", 
                  "while true; do\n", 
                  "  code=$(curl -H \"Accept: application/json\" -s -o /dev/null -w \"%{http_code}\" \"${zk_base_url}/cluster/status\")\n", 
                  "  if (( code == 200 )); then\n", 
                  "    num_serving=$(curl -H \"Accept: application/json\" \"${zk_base_url}/cluster/status\" 2>/dev/null | jq '[.[] | select(.description == \"serving\")] | length')\n", 
                  "\n", 
                  "    found=$(curl -H \"Accept: application/json\" ${zk_discovery_url} 2>/dev/null | jq \".servers | length\")\n", 
                  "\n", 
                  "    if (( $num_serving >= $num_servers && $found >= $num_servers )); then\n", 
                  "      # check servers\n", 
                  "      valid=()\n", 
                  "      for hp in $(zk-list-nodes | sed 's/,/\\n/g'); do\n",
                  "        host=$(echo $hp | sed 's/:.*$//')\n", 
                  "        exec 6<>/dev/tcp/$host/2181 && valid+=($host)\n", 
                  "      done\n", 
                  "      if (( ${#valid[@]}  >= $num_servers )); then\n", 
                  "        break\n", 
                  "      else\n",
                  "        echo -n \".\"\n",
                  "      fi\n", 
                  "    else\n", 
                  "      echo -n \".\"\n", 
                  "      sleep 1\n", 
                  "    fi\n", 
                  "  else\n", 
                  "    echo -n \".\"\n", 
                  "    sleep 1\n", 
                  "  fi\n", 
                  "done\n"
                ]]},
                "mode"    : "000755",
                "owner"   : "root",
                "group"   : "root"
              },
              "/usr/local/bin/provision" : {
                "content" : { "Fn::Join" : ["", [
                  "#!/bin/bash\n",
                  "# provision script\n",
                  "# provisioner puppet role1,role2,role3\n",
                  "# provisioner seed seed1 (s3://cell/seeds/seed1/*, untar * to /root/seeds/seed1, execute /root/seeds/seed1/*.sh)\n",
                  "source /etc/profile.d/cellos.sh\n",
                  "\n",
                  "if [[ $1 == \"puppet\" ]]; then\n",
                  "AWS_ACCESS_KEY_ID=\"${SAASBASE_ACCESS_KEY_ID}\" AWS_SECRET_ACCESS_KEY=\"${SAASBASE_SECRET_ACCESS_KEY}\"   bash /root/saasbase_installer -v -d /root -m /root/cluster/puppet/modules run-puppet /root/cluster --roles $2\n",
                  "elif [[ $1 == \"seed\" ]]; then\n",
                  "  [ -d /root/seeds/${2} ] && rm -rf /root/seeds/${2} && mkdir -p /root/seeds/${2}\n",
                  "  aws s3 cp s3://${cell_bucket_name}/seeds/${2} /root/seeds/${2}/\n",
                  "  pushd /root/seeds/${2}/\n",
                  "  for f in /root/seeds/${2}/*.tar.gz; do\n",
                  "    tar zxf $f\n",
                  "  done\n",
                  "  for e in /root/seeds/${2}/*.sh; do\n",
                  "    /bin/bash ${e}\n",
                  "  done\n", 
                  "  popd\n",
                  "fi\n",
                  "\n"
                ]]},
                "mode"    : "000755",
                "owner"   : "root",
                "group"   : "root"
              },
              "/usr/local/bin/detect-and-mount-disks" : {
                "content" : { "Fn::Join" : ["", [
                  "#!/bin/bash\n", 
                  "deviceslist=('/dev/xvdb' '/dev/xvdc' '/dev/xvdd' '/dev/xvde' '/dev/xvdf' '/dev/xvdg' '/dev/xvdh' '/dev/xvdi' '/dev/xvdj' '/dev/xvdk' '/dev/xvdl' '/dev/xvdm' '/dev/xvdn' '/dev/xvdo' '/dev/xvdp' '/dev/xvdq' '/dev/xvdr' '/dev/xvds' '/dev/xvdt' '/dev/xvdu' '/dev/xvdv' '/dev/xvdw' '/dev/xvdx' '/dev/xvdy' '/dev/xvdz')\n", 
                  "for device in ${deviceslist[@]}; do\n", 
                  "    if ([ -b $device ]) then\n", 
                  "        actual_devices=( \"${actual_devices[@]}\" $device )\n", 
                  "    fi\n", 
                  "done\n", 
                  "\n", 
                  "n=0\n", 
                  "for device in \"${actual_devices[@]}\"; do\n", 
                  "    n=`expr $n + 1`\n", 
                  "    if [ ! -b ${device}1 ]; then\n", 
                  "        echo \"Creating partition on ${device}\" >&2\n", 
                  "        parted -s -a optimal $device mklabel gpt -- mkpart primary xfs 1 -1 >&2\n", 
                  "        partprobe $device >&2 || true\n", 
                  "        mkfs.xfs ${device}1 >&2 || true\n", 
                  "    fi\n", 
                  "    if mountpoint -q -- \"/mnt/data_${n}\"; then\n", 
                  "        echo \"/mnt/data_${n} is already mounted\" >&2\n", 
                  "    else\n", 
                  "        echo \"$1 is not a mountpoint\" >&2\n", 
                  "        mkdir -p -m 755 /mnt/data_${n} >&2\n", 
                  "        mount ${device}1 /mnt/data_${n} >&2 || true\n", 
                  "        echo \"${device}1 /mnt/data_${n} xfs rw,relatime,attr2,inode64,noquota,nofail 0 0\" >> /etc/fstab\n", 
                  "    fi\n", 
                  "\n", 
                  "    mkdir -p -m 755 /mnt/data_${n}/{hadoop_data,kafka_data}\n", 
                  "    chown -R hadoop:hadoop /mnt/data_${n}/{hadoop_data,kafka_data}\n", 
                  "done\n", 
                  "echo $n\n"
                ]]},
                "mode"    : "000755",
                "owner"   : "root",
                "group"   : "root"
              }
            }
          }
        }
      },

      "Properties" : {
        "KeyName" : { "Ref" : "KeyName" },
        "ImageId" : { "Ref" : "ImageId" },
        "SecurityGroups" : { "Ref" : "SecurityGroups" },
        "BlockDeviceMappings": [
          {"VirtualName": "ephemeral0", "DeviceName": "/dev/sdb"}, 
          {"VirtualName": "ephemeral1", "DeviceName": "/dev/sdc"}, 
          {"VirtualName": "ephemeral2", "DeviceName": "/dev/sdd"}, 
          {"VirtualName": "ephemeral3", "DeviceName": "/dev/sde"}, 
          {"VirtualName": "ephemeral4", "DeviceName": "/dev/sdf"}, 
          {"VirtualName": "ephemeral5", "DeviceName": "/dev/sdg"}, 
          {"VirtualName": "ephemeral6", "DeviceName": "/dev/sdh"}, 
          {"VirtualName": "ephemeral7", "DeviceName": "/dev/sdi"}, 
          {"VirtualName": "ephemeral8", "DeviceName": "/dev/sdj"}, 
          {"VirtualName": "ephemeral9", "DeviceName": "/dev/sdk"}, 
          {"VirtualName": "ephemeral10", "DeviceName": "/dev/sdl"}, 
          {"VirtualName": "ephemeral11", "DeviceName": "/dev/sdm"}, 
          {"VirtualName": "ephemeral12", "DeviceName": "/dev/sdn"}, 
          {"VirtualName": "ephemeral13", "DeviceName": "/dev/sdo"}, 
          {"VirtualName": "ephemeral14", "DeviceName": "/dev/sdp"}, 
          {"VirtualName": "ephemeral15", "DeviceName": "/dev/sdq"}, 
          {"VirtualName": "ephemeral16", "DeviceName": "/dev/sdr"}, 
          {"VirtualName": "ephemeral17", "DeviceName": "/dev/sds"}, 
          {"VirtualName": "ephemeral18", "DeviceName": "/dev/sdt"}, 
          {"VirtualName": "ephemeral19", "DeviceName": "/dev/sdu"}, 
          {"VirtualName": "ephemeral20", "DeviceName": "/dev/sdv"}, 
          {"VirtualName": "ephemeral21", "DeviceName": "/dev/sdw"}, 
          {"VirtualName": "ephemeral22", "DeviceName": "/dev/sdx"}, 
          {"VirtualName": "ephemeral23", "DeviceName": "/dev/sdy"}
        ], 
        "IamInstanceProfile" : {
          "Fn::If": [
            "HasIamInstanceProfile",
            { "Ref": "IamInstanceProfile" },
            { "Ref": "AWS::NoValue" }
          ]
        },
        "AssociatePublicIpAddress": { "Ref" : "AssociatePublicIpAddress" },
        "InstanceType" : { "Ref" : "InstanceType" },
        "UserData"       : { "Fn::Base64" : { "Fn::Join" : ["", [
          "#!/bin/bash\n",
          "# default images mount the first ephemeral disk to /mnt\n", 
          "umount /mnt\n",
          "# set timezone\n", 
          "echo \"UTC\" > /etc/timezone\n", 
          "ln -sf /usr/share/zoneinfo/UTC /etc/localtime\n", 
          "# Base packages\n", 
          "yum install -y wget ruby ruby-devel kpartx parted\n", 

          "# Pip \n", 
          "curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n", 
          "python get-pip.py\n", 

          "# aws-cli \n", 
          "pip install --upgrade awscli\n", 
          "rpm -ivh http://s3.amazonaws.com/saasbase-repo/yumrepo/ec2-utils-0.6-2.el7.centos.noarch.rpm\n", 
          "rpm -ivh http://s3.amazonaws.com/saasbase-repo/yumrepo/ec2-net-utils-0.6-2.el7.centos.noarch.rpm\n", 
          "pip uninstall -y certifi\n", 
          "pip install certifi==2015.04.28\n", 

          "# cfn bootstrap \n", 
          "curl -O https://s3.amazonaws.com/cloudformation-examples/aws-cfn-bootstrap-latest.tar.gz\n",
          "tar -xzf aws-cfn-bootstrap-latest.tar.gz\n",
          "cd aws-cfn*\n",
          "easy_install pystache\n",
          "python setup.py install\n",

          "# Helper functions\n",
          "function error_exit\n",
          "{\n",
          "  cfn-signal -e 1 -r \"$1\" '", { "Ref" : "WaitHandle" }, "'\n",
          "  exit 1\n",
          "}\n",

          "# Process CloudFormation init definitions\n",
          "cfn-init -s ", { "Ref": "AWS::StackName" }, " -r BodyLaunchConfig  --region ", { "Ref": "AWS::Region" }, " || error_exit 'Failed to run cfn-init'\n",

          "# export vars\n", 
          "echo \"export number_of_disks=$(/usr/local/bin/detect-and-mount-disks)\" >> /etc/profile.d/cellos.sh\n",
          "source /etc/profile.d/cellos.sh\n", 
          "export pre_zk_modules='", { "Ref" : "PreZkModules" }, "'\n",
          "export post_zk_modules='", { "Ref" : "PostZkModules" }, "'\n",
          "export wait_handle='", { "Ref" : "WaitHandle" }, "'\n",

          "export search_instance_cmd=\"aws --region ${aws_region} ec2 describe-instances --query 'Reservations[*].Instances[*].[PrivateIpAddress, PrivateDnsName]' --filters Name=instance-state-code,Values=16 Name=tag:cell,Values=${cell_name}\"\n", 

          "# Provisioning \n", 
          "curl -o /root/saasbase_installer https://s3.amazonaws.com/saasbase-repo/saasbase_installer${saasbase_version}\n",
          "AWS_ACCESS_KEY_ID=\"${SAASBASE_ACCESS_KEY_ID}\" AWS_SECRET_ACCESS_KEY=\"${SAASBASE_SECRET_ACCESS_KEY}\" bash /root/saasbase_installer fetch ${saasbase_version}\n",
          "curl -o /root/puppet/profiles/${cellos_version}.yaml https://s3.amazonaws.com/saasbase-repo/cell-os/${cellos_version}.yaml", "\n",

          "mkdir -p /root/cluster/puppet/modules\n",
          "echo ${cellos_version} > /root/cluster/profile\n",
          "\n",
          "# pre zk\n",
          { "Ref" : "SaasBaseUserData" },
          "\n", 
          "echo 'zk-barrier'\n",
          "/usr/local/bin/zk-barrier\n",
          "export zk=`zk-list-nodes 2>/dev/null`\n",

          "# regenerate cluster.yaml\n",
          "echo '' > /root/cluster/cluster.yaml\n",
          "# post zk\n", 
          { "Ref" : "SaasBaseUserDataPost" },
          "\n", 
          "# All is well so signal success\n",
          "cfn-signal -e 0 -r \"Stack setup complete\" \"${wait_handle}\"\n",

          "#EOF"
        ]]}}
      }
    }

  }

}
